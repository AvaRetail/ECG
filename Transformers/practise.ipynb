{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install matplotlib transformers datasets opencv-python h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ATI-G2\\Envs\\trans\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import math\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGSequence():\n",
    "    @classmethod\n",
    "    def get_train_and_val(cls, path_to_hdf5, hdf5_dset, path_to_csv, batch_size=8, val_split=0.02):\n",
    "        n_samples = len(pd.read_csv(path_to_csv))\n",
    "        n_train = math.ceil(n_samples*(1-val_split))\n",
    "        train_seq = cls(path_to_hdf5, hdf5_dset, path_to_csv, batch_size, end_idx=n_train)\n",
    "        valid_seq = cls(path_to_hdf5, hdf5_dset, path_to_csv, batch_size, start_idx=n_train)\n",
    "        return train_seq, valid_seq\n",
    "\n",
    "    def __init__(self, path_to_hdf5, hdf5_dset, path_to_csv=None, batch_size=8,\n",
    "                 start_idx=0, end_idx=None):\n",
    "\n",
    "        # Get tracings\n",
    "        self.f = h5py.File(path_to_hdf5, \"r\")\n",
    "        self.x = self.f[hdf5_dset]\n",
    "        self.batch_size = batch_size\n",
    "        if end_idx is None:\n",
    "            end_idx = len(self.x)\n",
    "        self.start_idx = start_idx\n",
    "        self.end_idx = end_idx\n",
    "        # self.device = device\n",
    "        \n",
    "        if path_to_csv is None:\n",
    "            self.y = None\n",
    "        else:\n",
    "            exam_id = np.delete(np.array(self.f[\"exam_id\"]),np.where(np.array(self.f[\"exam_id\"])==0)).tolist()\n",
    "            pd_results = pd.read_csv(path_to_csv,index_col=\"exam_id\")\n",
    "            labels = pd_results.loc[exam_id,pd_results.columns[3:9]]\n",
    "\n",
    "            for label in labels.columns:\n",
    "                labels[label] = labels[label].apply(lambda x: 1 if x==True else 0)\n",
    "\n",
    "            self.y = labels.to_numpy()\n",
    "            self.exam_id = pd_results.index\n",
    "\n",
    "\n",
    "    @property\n",
    "    def n_classes(self):\n",
    "        return self.y.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = self.start_idx + idx * self.batch_size\n",
    "        end = min(start + self.batch_size, self.end_idx)\n",
    "        if self.y is None:\n",
    "            return np.array(self.x[start:end, :, :])\n",
    "        else:\n",
    "            # return np.array(self.x[start:end, :, :]), np.array(self.y[start:end])\n",
    "            return torch.Tensor(self.x[start:end, :, :]).permute(0,2,1), torch.from_numpy(self.y[start:end])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil((self.end_idx - self.start_idx) / self.batch_size)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1169160\n"
     ]
    }
   ],
   "source": [
    "# path_to_csv = r\"C:\\Users\\ATI-G2\\Documents\\python\\ECG\\data\\code-15\\exams.csv\"\n",
    "# pd_results = pd.read_csv(path_to_csv,index_col=\"exam_id\")\n",
    "# pd_results.index\n",
    "# for idx in pd_results.index:\n",
    "#     print(idx)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_data, val_data = ECGSequence.get_train_and_val(r\"C:\\Users\\ATI-G2\\Documents\\python\\ECG\\data\\code-15\\exams_part0.hdf5\",\n",
    "\"tracings\",r\"C:\\Users\\ATI-G2\\Documents\\python\\ECG\\data\\code-15\\exams.csv\",batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (data,_), label in zip(train_data, train_data.exam_id):\n",
    "\n",
    "\n",
    "    for i, signal in enumerate(data[0]):\n",
    "\n",
    "        plt.subplot(12,1,i+1)\n",
    "        plt.plot(np.arange(0,2000), signal[648:2648])\n",
    "\n",
    "        # Turn off spines and axis ticks\n",
    "        plt.gca().spines['top'].set_visible(False)\n",
    "        plt.gca().spines['bottom'].set_visible(False)\n",
    "        plt.gca().spines['left'].set_visible(False)\n",
    "        plt.gca().spines['right'].set_visible(False)\n",
    "        plt.gca().tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "        # Remove numbers along x and y-axes\n",
    "        plt.gca().xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        plt.gca().yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        \n",
    "    # plt.show()\n",
    "    plt.savefig(f\"..\\data\\code-15\\exams_part0_imgs\\img_{label}.jpg\", bbox_inches=\"tight\")\n",
    "    plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cv2.imwrite(\"img_cv2_224.jpg\",cv2.resize(cv2.imread(\"img.jpg\"),(224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your h5py file and extract necessary data\n",
    "# Example: Replace 'features' and 'labels' with your actual data\n",
    "with h5py.File('your_data.h5', 'r') as h5_file:\n",
    "    features = h5_file['features'][:]\n",
    "    labels = h5_file['labels'][:]\n",
    "\n",
    "# Create a Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'input_ids': features,\n",
    "    'labels': labels,\n",
    "})\n",
    "\n",
    "# Optionally set the format to 'torch' or 'tensorflow' depending on your preference\n",
    "dataset.set_format(type='torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-4.8781034e-01, -2.6677129e-01,  2.2103906e-01, ...,\n",
       "         -1.1737937e+00, -5.5640870e-01, -4.8781034e-01],\n",
       "        [-4.8106578e-01, -2.6019633e-01,  2.2086945e-01, ...,\n",
       "         -1.1674997e+00, -5.5151391e-01, -4.8232922e-01],\n",
       "        [-4.7979322e-01, -2.5791711e-01,  2.2187607e-01, ...,\n",
       "         -1.1599243e+00, -5.4229915e-01, -4.7729760e-01],\n",
       "        ...,\n",
       "        [-1.3724924e+00, -1.2811700e+00,  9.1322273e-02, ...,\n",
       "         -8.7824506e-01, -5.0396240e-01,  8.5938675e-04],\n",
       "        [-1.3667058e+00, -1.2758980e+00,  9.0807825e-02, ...,\n",
       "         -8.6861408e-01, -4.9709535e-01,  1.6935735e-03],\n",
       "        [-1.3614640e+00, -1.2694675e+00,  9.1996796e-02, ...,\n",
       "         -8.5737538e-01, -4.8235169e-01,  1.0566236e-02]],\n",
       "\n",
       "       [[ 4.7104187e+00,  3.4908926e+00, -1.2195258e+00, ...,\n",
       "         -2.7439332e-01,  6.4787310e-01,  1.1128174e+00],\n",
       "        [ 4.7069054e+00,  3.4883175e+00, -1.2185876e+00, ...,\n",
       "         -2.7435562e-01,  6.4121950e-01,  1.1126217e+00],\n",
       "        [ 4.7019477e+00,  3.4869583e+00, -1.2149894e+00, ...,\n",
       "         -2.7643222e-01,  6.3891876e-01,  1.1082902e+00],\n",
       "        ...,\n",
       "        [ 3.2337718e+00,  1.3490096e+00, -1.8847623e+00, ...,\n",
       "         -1.0202482e+00, -1.3055870e-01, -6.6891944e-01],\n",
       "        [ 3.2412338e+00,  1.3439417e+00, -1.8972923e+00, ...,\n",
       "         -1.0202823e+00, -1.2718490e-01, -6.7022973e-01],\n",
       "        [ 3.2448351e+00,  1.3389999e+00, -1.9058354e+00, ...,\n",
       "         -1.0213133e+00, -1.3284931e-01, -6.7119771e-01]],\n",
       "\n",
       "       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
       "\n",
       "       [[ 2.6677129e-01,  5.7927477e-01,  3.1250349e-01, ...,\n",
       "          1.0670851e-01,  8.3080190e-01, -2.6982012e+00],\n",
       "        [ 2.6778805e-01,  5.7443285e-01,  3.0664486e-01, ...,\n",
       "          9.0946034e-02,  8.2665092e-01, -2.7016087e+00],\n",
       "        [ 2.6224503e-01,  5.6280667e-01,  3.0056167e-01, ...,\n",
       "          7.3107675e-02,  8.1369811e-01, -2.7063200e+00],\n",
       "        ...,\n",
       "        [ 1.1951894e-01,  3.1130388e-01,  1.9178496e-01, ...,\n",
       "         -1.2996650e-01,  8.8944286e-01, -1.6984448e+00],\n",
       "        [ 1.2644379e-01,  3.1674707e-01,  1.9030328e-01, ...,\n",
       "         -1.2885189e-01,  8.9603120e-01, -1.6981733e+00],\n",
       "        [ 1.3006547e-01,  3.1913665e-01,  1.8907119e-01, ...,\n",
       "         -1.2993276e-01,  8.9771056e-01, -1.6980804e+00]],\n",
       "\n",
       "       [[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        ...,\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
       "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(f[\"tracings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "food = load_dataset('cifar10', split=['train[:5000]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food[0].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>,\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "\n",
    "lable2id, id2label = {}, {}\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    lable2id[label] = str(i)\n",
    "    id2label[f\"{i}\"] = label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "\n",
    "_transform = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(size),\n",
    "        ToTensor(),\n",
    "        normalize\n",
    "    ]\n",
    ")\n",
    "\n",
    "def transform(examples):\n",
    "    examples[\"pixel_value\"] = [_transform(img.convert(\"RGB\")) for img in examples[\"img\"]]\n",
    "    del examples[\"img\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=lable2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassifierOutput(loss=None, logits=tensor([[-0.1187,  0.1638,  0.0676,  0.0161, -0.0924,  0.0207,  0.0429, -0.0524,\n",
      "         -0.0219,  0.0347]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model.train()\n",
    "with torch.no_grad():\n",
    "    print(model(food[\"train\"][0][\"pixel_value\"].view(-1,3,224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "from torch import nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class ViTForImageClassification2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels=10):\n",
    "\n",
    "        super(ViTForImageClassification2, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        logits = self.classifier(outputs)\n",
    "        # probs = nn.Sigmoid()\n",
    "        loss = None\n",
    "\n",
    "        if labels is not None:\n",
    "\n",
    "          loss_fct = nn.BCEWithLogitsLoss()\n",
    "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
